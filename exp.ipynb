{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'roberta.pooler.dense.bias', 'lm_head.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.pooler.layer_weights', 'classifier.dense.bias', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import RobertaTokenizer\n",
    "from roberta import RobertaForSequenceClassification\n",
    "import torch\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "model = RobertaForSequenceClassification.from_pretrained(\"roberta-base\", output_hidden_states=True)\n",
    "\n",
    "inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
    "labels = torch.tensor([1]).unsqueeze(0)  # Batch size 1\n",
    "outputs = model(**inputs, output_hidden_states=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=None, logits=tensor([[ 0.0475, -0.0708]], grad_fn=<AddmmBackward0>), hidden_states=(tensor([[[ 0.1664, -0.0541, -0.0014,  ..., -0.0811,  0.0794,  0.0155],\n",
       "         [-0.4229,  0.1071, -0.3010,  ...,  0.0352, -0.3372,  0.2603],\n",
       "         [ 0.5254,  0.1029, -0.0767,  ..., -0.6114, -0.2440,  0.2591],\n",
       "         ...,\n",
       "         [ 0.2794,  0.0381, -0.0276,  ...,  0.1147, -0.0178, -0.0976],\n",
       "         [ 0.0204,  0.4912,  0.1750,  ...,  0.4872, -0.2833, -0.0511],\n",
       "         [ 0.1736, -0.1560,  0.0525,  ...,  0.3813,  0.1285,  0.1339]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), tensor([[[-4.5309e-02,  1.2841e-02,  6.1168e-02,  ...,  1.2439e-04,\n",
       "          -1.7222e-02, -1.0805e-01],\n",
       "         [-4.9035e-01, -8.0960e-01,  2.0653e-01,  ..., -2.1987e-01,\n",
       "          -3.9566e-01,  5.2398e-01],\n",
       "         [ 2.2331e-01, -7.4813e-02, -7.0698e-02,  ..., -1.1411e+00,\n",
       "          -1.1119e-01,  4.3990e-01],\n",
       "         ...,\n",
       "         [ 2.6296e-01, -1.9019e-01,  3.6889e-01,  ...,  4.9382e-02,\n",
       "          -2.2182e-01,  6.7661e-02],\n",
       "         [-3.7124e-01,  2.3186e-01,  4.2385e-01,  ...,  5.9505e-01,\n",
       "          -1.0074e+00,  3.5123e-03],\n",
       "         [ 7.6510e-02, -1.5070e-01,  4.5166e-01,  ...,  5.4768e-01,\n",
       "           5.7989e-02, -5.7921e-01]]], grad_fn=<NativeLayerNormBackward0>), tensor([[[ 0.0498,  0.0261, -0.0118,  ...,  0.0277, -0.0195, -0.0542],\n",
       "         [-0.5419, -0.5913, -0.1627,  ...,  0.1122, -0.3579,  0.3498],\n",
       "         [ 0.4223,  0.1942, -0.1123,  ..., -0.8358, -0.3345,  0.6133],\n",
       "         ...,\n",
       "         [-0.1317,  0.2040,  0.2335,  ...,  0.0867, -0.4717,  0.2111],\n",
       "         [-0.6625,  0.4531,  0.1510,  ...,  0.8153, -1.1790,  0.0881],\n",
       "         [-0.1281,  0.0462,  0.0563,  ...,  0.9048,  0.2116, -0.6232]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), tensor([[[ 0.0340,  0.0365, -0.0324,  ...,  0.0364,  0.0444,  0.0399],\n",
       "         [-0.0319, -0.2987,  0.1366,  ...,  0.2968, -0.0466,  0.0558],\n",
       "         [ 0.1674,  0.1452, -0.1071,  ..., -0.6148,  0.0630,  0.2647],\n",
       "         ...,\n",
       "         [-0.0549,  0.2798,  0.5075,  ...,  0.2898, -0.3573,  0.4559],\n",
       "         [-0.6432,  0.3929,  0.1931,  ...,  0.9578, -0.9716,  0.0961],\n",
       "         [-0.2538,  0.1770,  0.1049,  ...,  0.8630,  0.0331, -0.4717]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), tensor([[[ 0.0083,  0.0671, -0.0097,  ...,  0.0360, -0.0248, -0.0322],\n",
       "         [-0.2826, -0.1344,  0.1996,  ...,  0.2180, -0.1554, -0.0234],\n",
       "         [-0.2179,  0.3775,  0.0082,  ..., -0.6167,  0.0490,  0.1369],\n",
       "         ...,\n",
       "         [-0.3686,  0.1852,  0.2229,  ...,  0.2237, -0.4062,  0.3308],\n",
       "         [-0.7002,  0.3990,  0.1529,  ...,  0.8473, -0.8314,  0.1829],\n",
       "         [ 0.2306,  0.0456,  0.2049,  ...,  0.2226,  0.1912, -0.1266]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), tensor([[[-4.2592e-04,  1.0514e-01,  1.2406e-02,  ..., -2.2557e-03,\n",
       "           2.3898e-02,  5.1967e-02],\n",
       "         [-3.0742e-02, -3.1318e-01,  2.7403e-01,  ...,  2.3672e-01,\n",
       "          -3.4039e-01, -2.0186e-01],\n",
       "         [ 6.5379e-03,  2.6569e-01, -1.9255e-01,  ..., -5.8500e-01,\n",
       "           2.0868e-01,  3.7995e-02],\n",
       "         ...,\n",
       "         [-2.6698e-01,  3.9142e-01,  4.0423e-01,  ...,  5.6909e-02,\n",
       "          -1.3024e-01,  6.6182e-02],\n",
       "         [-5.0145e-01,  6.0304e-01,  2.7315e-01,  ...,  4.9411e-01,\n",
       "          -6.3645e-01, -1.2867e-01],\n",
       "         [ 5.7506e-02, -3.5413e-02,  6.2207e-02,  ...,  1.1474e-01,\n",
       "          -2.0134e-02,  1.1450e-02]]], grad_fn=<NativeLayerNormBackward0>), tensor([[[ 8.4833e-02,  7.3515e-02,  1.1009e-01,  ...,  2.2378e-02,\n",
       "          -1.5416e-02, -3.2401e-02],\n",
       "         [ 3.9331e-03,  1.6287e-02,  6.0952e-03,  ...,  1.0761e-01,\n",
       "          -3.1077e-01, -4.8785e-01],\n",
       "         [ 5.1321e-02,  2.9365e-01, -4.0065e-01,  ..., -6.3729e-01,\n",
       "           1.0847e-01, -1.9332e-01],\n",
       "         ...,\n",
       "         [ 4.7223e-02,  2.7136e-01, -2.1665e-02,  ...,  9.2003e-02,\n",
       "          -6.2781e-02, -1.1455e-01],\n",
       "         [-3.6399e-02,  7.2803e-01, -3.7197e-02,  ...,  4.8349e-01,\n",
       "          -3.3453e-01, -3.2218e-02],\n",
       "         [ 3.7163e-02,  5.1937e-03,  9.4846e-02,  ...,  3.4672e-04,\n",
       "          -5.7668e-02, -2.0398e-02]]], grad_fn=<NativeLayerNormBackward0>), tensor([[[-0.0108,  0.0844,  0.0627,  ...,  0.1251, -0.0342,  0.0138],\n",
       "         [-0.0350, -0.0764, -0.0515,  ...,  0.1240, -0.1203, -0.3517],\n",
       "         [-0.0422,  0.0683,  0.1219,  ..., -0.7114, -0.0059, -0.2766],\n",
       "         ...,\n",
       "         [ 0.3014,  0.3367,  0.1607,  ...,  0.0682, -0.1288, -0.0862],\n",
       "         [ 0.3334,  0.6304, -0.0408,  ...,  0.4874, -0.3719, -0.2044],\n",
       "         [ 0.0127,  0.0071,  0.0651,  ...,  0.0178,  0.0074,  0.0289]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), tensor([[[-1.0948e-01,  1.2757e-01,  1.0420e-02,  ...,  1.2072e-01,\n",
       "           2.2446e-02, -3.1604e-02],\n",
       "         [ 1.2611e-01,  6.0357e-02,  6.9243e-02,  ...,  4.1631e-01,\n",
       "          -4.3807e-01, -6.3101e-01],\n",
       "         [ 1.2711e-01,  2.0162e-01, -8.4233e-02,  ..., -3.2182e-01,\n",
       "           9.0117e-02, -5.6850e-01],\n",
       "         ...,\n",
       "         [ 2.1811e-01,  6.0163e-01,  1.9932e-01,  ...,  8.1858e-02,\n",
       "          -3.3706e-01, -4.1537e-01],\n",
       "         [ 2.8873e-01,  9.2758e-01,  3.6009e-02,  ...,  4.7342e-01,\n",
       "          -4.2169e-01, -2.1731e-01],\n",
       "         [ 8.2146e-04,  4.0399e-02,  2.6152e-02,  ...,  8.5935e-03,\n",
       "          -6.2801e-04, -9.9431e-03]]], grad_fn=<NativeLayerNormBackward0>), tensor([[[-8.6045e-02,  4.1524e-02, -1.1626e-01,  ...,  1.2701e-01,\n",
       "           4.5975e-02, -2.7339e-02],\n",
       "         [-5.3701e-02,  2.9082e-02, -1.6427e-01,  ...,  4.7388e-01,\n",
       "          -5.6919e-01, -2.9555e-01],\n",
       "         [ 4.2962e-01,  1.6103e-01, -5.3886e-02,  ..., -2.1000e-01,\n",
       "           1.0286e-01, -6.6340e-04],\n",
       "         ...,\n",
       "         [ 1.8374e-01,  7.5133e-01,  2.6152e-01,  ...,  2.6391e-01,\n",
       "          -1.1111e-01, -1.8614e-01],\n",
       "         [ 1.1871e-01,  4.2570e-01, -1.0245e-01,  ...,  7.1059e-01,\n",
       "           9.6509e-02,  1.0726e-01],\n",
       "         [ 4.2120e-02,  7.7366e-02,  1.3772e-01,  ..., -1.3649e-02,\n",
       "           1.2830e-01, -8.7909e-02]]], grad_fn=<NativeLayerNormBackward0>), tensor([[[-0.0024,  0.0181, -0.0749,  ...,  0.0820,  0.0768,  0.0144],\n",
       "         [ 0.4075, -0.0655, -0.4623,  ...,  0.2750, -0.1931, -0.4159],\n",
       "         [ 0.4321,  0.2134, -0.2134,  ..., -0.3003,  0.1591, -0.0137],\n",
       "         ...,\n",
       "         [-0.0240,  0.5531,  0.2500,  ...,  0.1706, -0.2725, -0.1013],\n",
       "         [ 0.0515,  0.2569,  0.0066,  ...,  0.6839, -0.0423,  0.1127],\n",
       "         [-0.0683,  0.0313,  0.0162,  ...,  0.0313,  0.0970, -0.0104]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), tensor([[[-0.0073, -0.0334,  0.0062,  ...,  0.0821, -0.0763,  0.0591],\n",
       "         [ 0.2282, -0.2033,  0.1107,  ...,  0.2199, -0.0334, -0.3296],\n",
       "         [ 0.3692,  0.2551, -0.0284,  ..., -0.1725,  0.1952, -0.1002],\n",
       "         ...,\n",
       "         [-0.0274,  0.5019,  0.1576,  ...,  0.2170, -0.0387, -0.0299],\n",
       "         [ 0.2610,  0.0049,  0.1318,  ...,  0.5754, -0.2224,  0.0975],\n",
       "         [-0.0439, -0.0199,  0.0482,  ...,  0.0145, -0.0143,  0.0243]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), tensor([[[-0.0478,  0.0886, -0.0098,  ..., -0.0544, -0.0672, -0.0039],\n",
       "         [-0.0712,  0.0150, -0.1299,  ...,  0.0638,  0.0296, -0.0860],\n",
       "         [ 0.0906,  0.1437,  0.0828,  ...,  0.0509, -0.0320, -0.0490],\n",
       "         ...,\n",
       "         [ 0.0853,  0.2155,  0.0849,  ..., -0.1150,  0.0330, -0.0790],\n",
       "         [ 0.1679,  0.1288,  0.0065,  ...,  0.0367, -0.0631,  0.0276],\n",
       "         [-0.0436,  0.0892, -0.0389,  ..., -0.0957, -0.0744, -0.0284]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>)), attentions=None)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "efa909a5840425ae48d49c23e1da582ebaa3b81076302bec2bb6bab8125582c7"
  },
  "kernelspec": {
   "display_name": "Python 3.7.9 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
